{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4420eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "PAGE_IMAGES_FOLDER = \"D:\\OCR session\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1bd2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.11.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.15.3)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (12.0.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2025.5.10)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c14dab",
   "metadata": {},
   "source": [
    "---\n",
    "## Tilt correction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac9a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.transform import hough_line, hough_line_peaks\n",
    "from skimage.feature import canny\n",
    "\n",
    "WATERMARK_THRESHOLD_LOW = 175\n",
    "WATERMARK_THRESHOLD_HIGH = 250\n",
    "\n",
    "# Model for orientation skew correction\n",
    "EAST_MODEL = \"frozen_east_text_detection.pb\"\n",
    "ANGLE_TOLLERANCE = 0.25\n",
    "MIN_CONFIDENCE = 0.5\n",
    "MARGIN_TOLLERANCE = 9\n",
    "EAST_WIDTH = 1280\n",
    "EAST_HEIGHT = 1280\n",
    "ALIGN = False\n",
    "ALIGN_MODE = 'FAST'\n",
    "\n",
    "class Orientation:\n",
    "\n",
    "    def __init__(self, image, file_properties, conf_threshold=50, lang='eng'):\n",
    "\n",
    "        # self.image_path     = image_path\n",
    "        self.image = image\n",
    "        self.file_properties = file_properties\n",
    "        # self.lines          = lines\n",
    "        self.conf_threshold = int(conf_threshold)\n",
    "\n",
    "        self.timer = {'net': 0, 'restore': 0, 'nms': 0}\n",
    "        self.text = {}\n",
    "        self.lang = lang\n",
    "\n",
    "        # self.re_orient()\n",
    "\n",
    "    def rotate_bound(self, image, angle):\n",
    "        # grab the dimensions of the image and then determine the\n",
    "        # center\n",
    "        (h, w) = image.shape[:2]\n",
    "        (cX, cY) = (w / 2, h / 2)\n",
    "\n",
    "        # grab the rotation matrix (applying the negative of the\n",
    "        # angle to rotate clockwise), then grab the sine and cosine\n",
    "        # (i.e., the rotation components of the matrix)\n",
    "        M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
    "        cos = np.abs(M[0, 0])\n",
    "        sin = np.abs(M[0, 1])\n",
    "\n",
    "        # compute the new bounding dimensions of the image\n",
    "        nW = int((h * sin) + (w * cos))\n",
    "        nH = int((h * cos) + (w * sin))\n",
    "\n",
    "        # adjust the rotation matrix to take into account translation\n",
    "        M[0, 2] += (nW / 2) - cX\n",
    "        M[1, 2] += (nH / 2) - cY\n",
    "\n",
    "        # perform the actual rotation and return the image\n",
    "        return cv2.warpAffine(image, M, (nW, nH), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    def east_detect(self, image, args):\n",
    "\n",
    "        # orig = image.copy()\n",
    "        (H, W) = image.shape[:2]\n",
    "\n",
    "        (newW, newH) = (args[\"width\"], args[\"height\"])\n",
    "        rW = W / float(newW)\n",
    "        rH = H / float(newH)\n",
    "\n",
    "        image = cv2.resize(image, (newW, newH))\n",
    "        (H, W) = image.shape[:2]\n",
    "\n",
    "        layerNames = [\n",
    "            \"feature_fusion/Conv_7/Sigmoid\",\n",
    "            \"feature_fusion/concat_3\"]\n",
    "\n",
    "        # print(\"[INFO] loading EAST text detector...\")\n",
    "        net = cv2.dnn.readNet(args[\"east\"])\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "                                     (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "        # start = time.time()\n",
    "        net.setInput(blob)\n",
    "        (scores, geometry) = net.forward(layerNames)\n",
    "        # end = time.time()\n",
    "\n",
    "        # print(\"[INFO] text detection took {:.6f} seconds\".format(end - start))\n",
    "\n",
    "        # confidence scores\n",
    "        (numRows, numCols) = scores.shape[2:4]\n",
    "        angl = []\n",
    "\n",
    "        for y in range(0, numRows):\n",
    "\n",
    "            scoresData = scores[0, 0, y]\n",
    "            anglesData = geometry[0, 4, y]\n",
    "\n",
    "            for x in range(0, numCols):\n",
    "                if scoresData[x] < args[\"min_confidence\"]:\n",
    "                    continue\n",
    "\n",
    "                angle = anglesData[x]\n",
    "                angl.append(angle*180/(np.pi))\n",
    "\n",
    "        return np.median(angl)\n",
    "\n",
    "    def east(self, image, args):\n",
    "\n",
    "        # image = cv2.imread(image_path)\n",
    "        angle = Orientation.east_detect(self, image, args)\n",
    "        # print(\"angle*********\",angle)\n",
    "\n",
    "        return image, angle\n",
    "\n",
    "    def hough_transforms(self, image):\n",
    "\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        thresh = cv2.GaussianBlur(gray, (11, 11), 0)\n",
    "        edges = canny(thresh)\n",
    "        tested_angles = np.deg2rad(np.arange(0.1, 180.0))\n",
    "        h, theta, d = hough_line(edges, theta=tested_angles)\n",
    "        accum, angles, dists = hough_line_peaks(h, theta, d)\n",
    "\n",
    "        return accum, angles, dists\n",
    "\n",
    "    def east_hough_line(self, image, args):\n",
    "        image, angle = Orientation.east(self, image, args)\n",
    "        h, theta, d = Orientation.hough_transforms(self, image)\n",
    "        theta = np.rad2deg(np.pi/2-theta)\n",
    "        # theta = np.rad2deg(theta-np.pi/2)\n",
    "        margin = args['margin_tollerance']\n",
    "        low_thresh = angle-margin\n",
    "        high_thresh = angle+margin\n",
    "        filter_theta = theta[theta > low_thresh]\n",
    "        filter_theta = filter_theta[filter_theta < high_thresh]\n",
    "\n",
    "        return image, np.median(filter_theta)\n",
    "\n",
    "    def re_orient_east(self):\n",
    "        lang = 'hi'\n",
    "\n",
    "        args = {\n",
    "            \"image\": self.image,\n",
    "            \"east\": EAST_MODEL,\n",
    "            \"min_confidence\": MIN_CONFIDENCE,\n",
    "            \"margin_tollerance\": MARGIN_TOLLERANCE,\n",
    "            \"width\": EAST_WIDTH,\n",
    "            \"height\": EAST_HEIGHT\n",
    "        }\n",
    "\n",
    "        image, angle = Orientation.east_hough_line(self, args['image'], args)\n",
    "\n",
    "        if abs(angle) > ANGLE_TOLLERANCE:\n",
    "            image = Orientation.rotate_bound(self, image, angle)\n",
    "            # print(self.image_path)\n",
    "            # image_path = Orientation(self.image_path)\n",
    "            # cv2.imwrite(f'{self.image_path}', image)\n",
    "\n",
    "        print(\"Angle detectd is  {} \".format(angle))\n",
    "\n",
    "        return image, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a14872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: doclayout-yolo in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from doclayout-yolo) (3.10.6)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (12.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (1.15.3)\n",
      "Requirement already satisfied: torch>=2.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (2.2.0+cpu)\n",
      "Requirement already satisfied: torchvision>=0.15.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (0.17.0+cpu)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from doclayout-yolo) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from doclayout-yolo) (2.3.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from doclayout-yolo) (0.13.2)\n",
      "Requirement already satisfied: albumentations>=1.4.11 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from doclayout-yolo) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations>=1.4.11->doclayout-yolo) (1.26.4)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations>=1.4.11->doclayout-yolo) (2.11.7)\n",
      "Requirement already satisfied: albucore==0.0.24 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from albumentations>=1.4.11->doclayout-yolo) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations>=1.4.11->doclayout-yolo) (4.10.0.84)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albucore==0.0.24->albumentations>=1.4.11->doclayout-yolo) (4.0.0)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albucore==0.0.24->albumentations>=1.4.11->doclayout-yolo) (6.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->doclayout-yolo) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->doclayout-yolo) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->doclayout-yolo) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->doclayout-yolo) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->doclayout-yolo) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->doclayout-yolo) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->doclayout-yolo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.4->doclayout-yolo) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.4->doclayout-yolo) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.9.2->albumentations>=1.4.11->doclayout-yolo) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.9.2->albumentations>=1.4.11->doclayout-yolo) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.9.2->albumentations>=1.4.11->doclayout-yolo) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.9.2->albumentations>=1.4.11->doclayout-yolo) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->doclayout-yolo) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->doclayout-yolo) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->doclayout-yolo) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->doclayout-yolo) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->doclayout-yolo) (2025.8.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.1->doclayout-yolo) (3.19.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.1->doclayout-yolo) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.1->doclayout-yolo) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.1->doclayout-yolo) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.1->doclayout-yolo) (2025.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.64.0->doclayout-yolo) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=2.0.1->doclayout-yolo) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=2.0.1->doclayout-yolo) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install doclayout-yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc6b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: doclayout_yolo\n",
      "Version: 0.0.2\n",
      "Summary: DocLayout-YOLO: an effecient and robust document layout analysis method.\n",
      "Home-page: \n",
      "Author: Zhiyuan Zhao, Hengrui Kang, Bin Wang, Conghui He\n",
      "Author-email: \n",
      "License: AGPL-3.0\n",
      "Location: c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\n",
      "Requires: albumentations, matplotlib, opencv-python, pandas, pillow, psutil, py-cpuinfo, pyyaml, requests, scipy, seaborn, thop, torch, torchvision, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show doclayout-yolo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beac2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = r\"D:\\OCR\\Pipeline\\yolov12l-doclaynet.pt\"\n",
    "model = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4201b6",
   "metadata": {},
   "source": [
    "---\n",
    "## Layout detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2c1333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import Counter, defaultdict\n",
    "from skimage.transform import hough_line, hough_line_peaks\n",
    "from skimage.feature import canny\n",
    "from doclayout_yolo import YOLOv10  # Ensure doclayout-yolo is installed\n",
    "\n",
    "# ---------- Model Setup ----------\n",
    "model_path = r\"D:\\OCR\\Pipeline\\yolov12l-doclaynet.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "def iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "    xi1, yi1 = max(x1, x1_p), max(y1, y1_p)\n",
    "    xi2, yi2 = min(x2, x2_p), min(y2, y2_p)\n",
    "    iw = max(0.0, xi2 - xi1)\n",
    "    ih = max(0.0, yi2 - yi1)\n",
    "    inter = iw * ih\n",
    "    area1 = max(0.0, (x2 - x1) * (y2 - y1))\n",
    "    area2 = max(0.0, (x2_p - x1_p) * (y2_p - y1_p))\n",
    "    union = area1 + area2 - inter\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def perform_prediction(image, model, imgsz=1024, conf=0.4, iou=0.45, device=\"cpu\"):\n",
    "    return model.predict(image, imgsz=imgsz, conf=conf, iou=iou, device=device)\n",
    "\n",
    "def extract_bboxes(det_res_list):\n",
    "    boxes_info = []\n",
    "    for result in det_res_list:\n",
    "        names = result.names\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "            label_idx = int(box.cls.item())\n",
    "            confidence = float(box.conf.item())\n",
    "            label = names.get(label_idx, str(label_idx))\n",
    "            boxes_info.append({\"bbox\": [x1, y1, x2, y2], \"label\": label, \"confidence\": confidence})\n",
    "    return boxes_info\n",
    "\n",
    "def nms_classwise(boxes, iou_thresh=0.3):  # Less aggressive\n",
    "    out = []\n",
    "    labels = set([b[\"label\"] for b in boxes])\n",
    "    for label in labels:\n",
    "        group = [b for b in boxes if b[\"label\"] == label]\n",
    "        group = sorted(group, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "        keep = []\n",
    "        suppressed = [False] * len(group)\n",
    "        for i in range(len(group)):\n",
    "            if suppressed[i]:\n",
    "                continue\n",
    "            keep.append(group[i])\n",
    "            for j in range(i + 1, len(group)):\n",
    "                if suppressed[j]:\n",
    "                    continue\n",
    "                if iou(group[i][\"bbox\"], group[j][\"bbox\"]) > iou_thresh:\n",
    "                    suppressed[j] = True\n",
    "        out.extend(keep)\n",
    "    return out\n",
    "\n",
    "def deduplicate_boxes(boxes, iou_thresh=0.2):  # Less aggressive\n",
    "    out = []\n",
    "    labels = set([b[\"label\"] for b in boxes])\n",
    "    for label in labels:\n",
    "        group = [b for b in boxes if b[\"label\"] == label]\n",
    "        group = sorted(group, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "        keep = []\n",
    "        while group:\n",
    "            ref = group.pop(0)\n",
    "            keep.append(ref)\n",
    "            group = [g for g in group if iou(ref[\"bbox\"], g[\"bbox\"]) < iou_thresh]\n",
    "        out.extend(keep)\n",
    "    return out\n",
    "\n",
    "def merge_text_boxes(boxes, merge_iou=0.1):  # Only mildly merge\n",
    "    text_boxes = [b for b in boxes if b[\"label\"].lower() == \"text\"]\n",
    "    other_boxes = [b for b in boxes if b[\"label\"].lower() != \"text\"]\n",
    "    if not text_boxes:\n",
    "        return boxes\n",
    "    used = [False] * len(text_boxes)\n",
    "    merged = []\n",
    "    for i in range(len(text_boxes)):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        bx = text_boxes[i][\"bbox\"].copy()\n",
    "        conf = text_boxes[i][\"confidence\"]\n",
    "        used[i] = True\n",
    "        for j in range(i+1, len(text_boxes)):\n",
    "            if used[j]:\n",
    "                continue\n",
    "            if iou(bx, text_boxes[j][\"bbox\"]) >= merge_iou:\n",
    "                bx = [\n",
    "                    min(bx[0], text_boxes[j][\"bbox\"][0]),\n",
    "                    min(bx[1], text_boxes[j][\"bbox\"][1]),\n",
    "                    max(bx[2], text_boxes[j][\"bbox\"][2]),\n",
    "                    max(bx[3], text_boxes[j][\"bbox\"][3]),\n",
    "                ]\n",
    "                conf = max(conf, text_boxes[j][\"confidence\"])\n",
    "                used[j] = True\n",
    "        merged.append({\"bbox\": bx, \"label\": \"Text\", \"confidence\": conf})\n",
    "    return other_boxes + merged\n",
    "\n",
    "def resolve_cross_class_conflicts(boxes, cross_iou_thresh=0.9999, class_priority=None):\n",
    "    if class_priority is None:\n",
    "        class_priority = defaultdict(lambda: 0)\n",
    "        class_priority.update({\"Text\": 6, \"Table\": 5, \"Picture\": 4,\n",
    "                              \"Section-header\": 3, \"List-item\": 2, \"Caption\": 1})\n",
    "    boxes_sorted = sorted(boxes, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "    keep = []\n",
    "    removed = [False] * len(boxes_sorted)\n",
    "    for i in range(len(boxes_sorted)):\n",
    "        if removed[i]:\n",
    "            continue\n",
    "        a = boxes_sorted[i]\n",
    "        keep.append(a)\n",
    "        for j in range(i+1, len(boxes_sorted)):\n",
    "            if removed[j]:\n",
    "                continue\n",
    "            b = boxes_sorted[j]\n",
    "            pair_iou = iou(a[\"bbox\"], b[\"bbox\"])\n",
    "            if pair_iou >= cross_iou_thresh:\n",
    "                if b[\"confidence\"] > a[\"confidence\"]:\n",
    "                    keep[-1] = b\n",
    "                    removed[i] = True\n",
    "                    removed[j] = True\n",
    "                elif abs(b[\"confidence\"] - a[\"confidence\"]) < 1e-6:\n",
    "                    if class_priority.get(b[\"label\"],0) > class_priority.get(a[\"label\"],0):\n",
    "                        keep[-1] = b\n",
    "                removed[j] = True\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for item in keep:\n",
    "        key = tuple([round(x, 2) for x in item[\"bbox\"]]) + (item[\"label\"],)\n",
    "        if key not in seen:\n",
    "            uniq.append(item)\n",
    "            seen.add(key)\n",
    "    return uniq\n",
    "\n",
    "def visualize_bboxes(image, boxes, title=None, save_path=None):\n",
    "    img = np.array(image.convert(\"RGB\"))\n",
    "    label_colors = {}\n",
    "    palette = [(0,255,0),(0,128,255),(255,0,0),(255,128,0),(128,0,255),(0,200,200)]\n",
    "    for i,b in enumerate(sorted(list(set([x[\"label\"] for x in boxes])))):\n",
    "        label_colors[b] = palette[i % len(palette)]\n",
    "    for b in boxes:\n",
    "        x1,y1,x2,y2 = map(int, b[\"bbox\"])\n",
    "        lbl = b[\"label\"]\n",
    "        conf = b[\"confidence\"]\n",
    "        color = label_colors.get(lbl, (0,255,0))\n",
    "        cv2.rectangle(img, (x1,y1),(x2,y2), color, 2)\n",
    "        txt = f\"{lbl} {conf:.2f}\"\n",
    "        cv2.putText(img, txt, (x1, max(0,y1-8)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2, lineType=cv2.LINE_AA)\n",
    "    out = Image.fromarray(img)\n",
    "    if save_path:\n",
    "        out.save(save_path)\n",
    "    return out\n",
    "\n",
    "def process_image_with_yolo(model, image_path,\n",
    "                            conf=0.5, model_iou=0.45,\n",
    "                            pre_conf_thresh=0.5,\n",
    "                            class_nms_iou=0.3,      # Lowered\n",
    "                            merge_text_iou=0.1,     # Lowered\n",
    "                            cross_iou_thresh=0.9999,\n",
    "                            final_conf_thresh=0.5,\n",
    "                            dedup_iou=0.2,          # Lowered\n",
    "                            visualize=True,\n",
    "                            out_before=\"before.png\", out_after=\"after.png\"):\n",
    "\n",
    "    pil_img = Image.open(image_path).convert(\"RGB\")\n",
    "    raw_results = perform_prediction(pil_img, model, imgsz=1024, conf=conf, iou=model_iou, device=\"cpu\")\n",
    "\n",
    "    boxes = extract_bboxes(raw_results)\n",
    "    counter_before = Counter([b[\"label\"] for b in boxes])\n",
    "    print(\"Before:\", dict(counter_before), \" total:\", len(boxes))\n",
    "    if visualize:\n",
    "        visualize_bboxes(pil_img, boxes, save_path=out_before)\n",
    "\n",
    "    # 1) Pre-filter\n",
    "    boxes = [b for b in boxes if b[\"confidence\"] >= pre_conf_thresh]\n",
    "    print(\"Post pre-filter:\", Counter([b[\"label\"] for b in boxes]))\n",
    "\n",
    "    # 2) Class-wise NMS\n",
    "    boxes = nms_classwise(boxes, iou_thresh=class_nms_iou)\n",
    "    print(\"Post NMS:\", Counter([b[\"label\"] for b in boxes]))\n",
    "\n",
    "    # 3) Merge text (only if 'Text' boxes exist)\n",
    "    if any(b[\"label\"].lower() == \"text\" for b in boxes):\n",
    "        boxes = merge_text_boxes(boxes, merge_iou=merge_text_iou)\n",
    "    print(\"Post text merge:\", Counter([b[\"label\"] for b in boxes]))\n",
    "\n",
    "    # 4) Cross-class resolution\n",
    "    boxes = resolve_cross_class_conflicts(boxes, cross_iou_thresh=cross_iou_thresh)\n",
    "    print(\"Post cross-class:\", Counter([b[\"label\"] for b in boxes]))\n",
    "\n",
    "    # 5) Extra deduplication per class\n",
    "    boxes = deduplicate_boxes(boxes, iou_thresh=dedup_iou)\n",
    "    print(\"Post deduplication:\", Counter([b[\"label\"] for b in boxes]))\n",
    "\n",
    "    # 6) Final filter\n",
    "    boxes = [b for b in boxes if b[\"confidence\"] >= final_conf_thresh]\n",
    "    counter_after = Counter([b[\"label\"] for b in boxes])\n",
    "    print(\"After:\", dict(counter_after), \" total:\", len(boxes))\n",
    "    if visualize:\n",
    "        visualize_bboxes(pil_img, boxes, save_path=out_after)\n",
    "\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d185b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 672x1024 1 Caption, 2 Page-footers, 2 Pictures, 1 Section-header, 20 Texts, 5759.3ms\n",
      "Speed: 22.1ms preprocess, 5759.3ms inference, 7.7ms postprocess per image at shape (1, 3, 672, 1024)\n",
      "Before: {'Text': 20, 'Picture': 2, 'Caption': 1, 'Section-header': 1, 'Page-footer': 2}  total: 26\n",
      "Post pre-filter: Counter({'Text': 20, 'Picture': 2, 'Page-footer': 2, 'Caption': 1, 'Section-header': 1})\n",
      "Post NMS: Counter({'Text': 20, 'Picture': 2, 'Page-footer': 2, 'Caption': 1, 'Section-header': 1})\n",
      "Post text merge: Counter({'Text': 20, 'Picture': 2, 'Page-footer': 2, 'Caption': 1, 'Section-header': 1})\n",
      "Post cross-class: Counter({'Text': 20, 'Picture': 2, 'Page-footer': 2, 'Caption': 1, 'Section-header': 1})\n",
      "Post deduplication: Counter({'Text': 20, 'Picture': 2, 'Page-footer': 2, 'Caption': 1, 'Section-header': 1})\n",
      "After: {'Text': 20, 'Picture': 2, 'Caption': 1, 'Section-header': 1, 'Page-footer': 2}  total: 26\n",
      "[\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            245.37193298339844,\n",
      "            428.94195556640625,\n",
      "            607.8034057617188,\n",
      "            559.9708251953125\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9709159731864929\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            249.5948486328125,\n",
      "            57.509254455566406,\n",
      "            611.5081176757812,\n",
      "            208.0161590576172\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.97053062915802\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            627.24267578125,\n",
      "            609.5105590820312,\n",
      "            974.8970947265625,\n",
      "            797.5830078125\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.970015287399292\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1403.0296630859375,\n",
      "            622.1387939453125,\n",
      "            1766.1734619140625,\n",
      "            868.12890625\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9696635603904724\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            248.0624237060547,\n",
      "            215.09664916992188,\n",
      "            610.1593627929688,\n",
      "            422.609375\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9683952927589417\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            634.1262817382812,\n",
      "            180.70947265625,\n",
      "            980.533203125,\n",
      "            368.48175048828125\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9679659605026245\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            631.3729248046875,\n",
      "            376.0761413574219,\n",
      "            977.5526123046875,\n",
      "            602.19970703125\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9671081304550171\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1403.93994140625,\n",
      "            874.6943359375,\n",
      "            1768.267822265625,\n",
      "            1004.8417358398438\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.965542733669281\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1031.06689453125,\n",
      "            945.8391723632812,\n",
      "            1380.67578125,\n",
      "            1057.491943359375\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9643125534057617\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1030.433837890625,\n",
      "            769.666015625,\n",
      "            1379.899169921875,\n",
      "            939.109619140625\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.963257372379303\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1032.45947265625,\n",
      "            163.05995178222656,\n",
      "            1377.121337890625,\n",
      "            294.0759582519531\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9624536037445068\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            621.8131713867188,\n",
      "            1019.15283203125,\n",
      "            970.157470703125,\n",
      "            1113.1953125\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9596273899078369\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1405.1710205078125,\n",
      "            1030.6322021484375,\n",
      "            1770.570068359375,\n",
      "            1103.0804443359375\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.953392744064331\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1034.1639404296875,\n",
      "            67.07084655761719,\n",
      "            1376.5589599609375,\n",
      "            140.2308807373047\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9522997736930847\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            635.719970703125,\n",
      "            62.957054138183594,\n",
      "            981.8026733398438,\n",
      "            154.5970916748047\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.949093759059906\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1045.3389892578125,\n",
      "            317.0807189941406,\n",
      "            1365.0792236328125,\n",
      "            729.754150390625\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9231075644493103\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1031.4571533203125,\n",
      "            1083.0184326171875,\n",
      "            1380.5452880859375,\n",
      "            1117.33935546875\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.9223970174789429\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            624.5645751953125,\n",
      "            875.7650146484375,\n",
      "            972.9749145507812,\n",
      "            941.7012329101562\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.879671037197113\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            626.7080078125,\n",
      "            821.403564453125,\n",
      "            972.7593383789062,\n",
      "            857.7972412109375\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.873308539390564\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            245.67852783203125,\n",
      "            1062.4644775390625,\n",
      "            542.7977905273438,\n",
      "            1104.6025390625\n",
      "        ],\n",
      "        \"label\": \"Text\",\n",
      "        \"confidence\": 0.7255280613899231\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1432.3441162109375,\n",
      "            66.1109619140625,\n",
      "            1719.1168212890625,\n",
      "            489.9978332519531\n",
      "        ],\n",
      "        \"label\": \"Picture\",\n",
      "        \"confidence\": 0.9095626473426819\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            273.7344970703125,\n",
      "            817.6224975585938,\n",
      "            561.278564453125,\n",
      "            1046.154052734375\n",
      "        ],\n",
      "        \"label\": \"Picture\",\n",
      "        \"confidence\": 0.6062057018280029\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1413.6287841796875,\n",
      "            501.29608154296875,\n",
      "            1742.8985595703125,\n",
      "            558.8482666015625\n",
      "        ],\n",
      "        \"label\": \"Caption\",\n",
      "        \"confidence\": 0.9034200310707092\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            622.6539306640625,\n",
      "            988.43115234375,\n",
      "            800.9111328125,\n",
      "            1012.9816284179688\n",
      "        ],\n",
      "        \"label\": \"Section-header\",\n",
      "        \"confidence\": 0.8526953458786011\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1246.3453369140625,\n",
      "            1167.4722900390625,\n",
      "            1359.3441162109375,\n",
      "            1180.74169921875\n",
      "        ],\n",
      "        \"label\": \"Page-footer\",\n",
      "        \"confidence\": 0.7109100222587585\n",
      "    },\n",
      "    {\n",
      "        \"bbox\": [\n",
      "            1426.5250244140625,\n",
      "            1165.76123046875,\n",
      "            1612.271484375,\n",
      "            1178.9266357421875\n",
      "        ],\n",
      "        \"label\": \"Page-footer\",\n",
      "        \"confidence\": 0.6899243593215942\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from doclayout_yolo import YOLO\n",
    "\n",
    "# Load model\n",
    "model = YOLO(\"D:\\OCR\\Pipeline\\yolov12l-doclaynet.pt\")   # update with your model path\n",
    "\n",
    "# Run pipeline\n",
    "boxes = process_image_with_yolo(\n",
    "    model=model,\n",
    "    image_path= \"D:\\OCR session\\pic 14.jpg\",   # replace with your image\n",
    "    conf=0.5,\n",
    "    model_iou=0.45,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# Convert result to JSON\n",
    "boxes_json = json.dumps(boxes, indent=4)\n",
    "\n",
    "# Print JSON\n",
    "print(boxes_json)\n",
    "\n",
    "# Optionally save to file\n",
    "with open(\"output_boxes.json\", \"w\") as f:\n",
    "    f.write(boxes_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d69db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final \"after.png\" result:\n",
    "cleaned_img = Image.open(\"after.png\")\n",
    "cleaned_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "885c92ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv12l summary: 488 layers, 26,397,585 parameters, 0 gradients, 89.5 GFLOPs\n",
      "(488, 26397585, 0, 89.4527744)\n"
     ]
    }
   ],
   "source": [
    "print(model.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fb1f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving paragraph crops\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2dbbd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total page images: 25\n",
      ">>> Processing image: pic 1.png\n",
      "\n",
      "0: 288x1024 1 Table, 1470.4ms\n",
      "Speed: 7.6ms preprocess, 1470.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 1024)\n",
      "Total crops: 0\n",
      ">>> Processing image: pic 10.jpg\n",
      "\n",
      "0: 1024x672 2 Pictures, 1 Section-header, 5 Texts, 3410.7ms\n",
      "Speed: 14.7ms preprocess, 3410.7ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 672)\n",
      "Total crops: 8\n",
      "Saving image crop 0... label=Picture conf=0.897\n",
      "Saving image crop 1... label=Picture conf=0.894\n",
      "Saving image crop 2... label=Text conf=0.949\n",
      "Saving image crop 3... label=Text conf=0.920\n",
      "Saving image crop 4... label=Text conf=0.889\n",
      "Saving image crop 5... label=Text conf=0.793\n",
      "Saving image crop 6... label=Text conf=0.738\n",
      "Saving image crop 7... label=Section-header conf=0.805\n",
      ">>> Processing image: pic 11.png\n",
      "\n",
      "0: 352x1024 2 Tables, 1342.1ms\n",
      "Speed: 0.0ms preprocess, 1342.1ms inference, 0.0ms postprocess per image at shape (1, 3, 352, 1024)\n",
      "Total crops: 0\n",
      ">>> Processing image: pic 12.jpg\n",
      "\n",
      "0: 1024x768 2 Page-footers, 2 Pictures, 6 Texts, 3235.5ms\n",
      "Speed: 22.0ms preprocess, 3235.5ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 768)\n",
      "Total crops: 10\n",
      "Saving image crop 0... label=Picture conf=0.907\n",
      "Saving image crop 1... label=Picture conf=0.616\n",
      "Saving image crop 2... label=Page-footer conf=0.749\n",
      "Saving image crop 3... label=Page-footer conf=0.734\n",
      "Saving image crop 4... label=Text conf=0.952\n",
      "Saving image crop 5... label=Text conf=0.930\n",
      "Saving image crop 6... label=Text conf=0.925\n",
      "Saving image crop 7... label=Text conf=0.914\n",
      "Saving image crop 8... label=Text conf=0.888\n",
      "Saving image crop 9... label=Text conf=0.843\n",
      ">>> Processing image: pic 13.png\n",
      "\n",
      "0: 512x1024 1 Picture, 1 Text, 2015.5ms\n",
      "Speed: 9.1ms preprocess, 2015.5ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 1024)\n",
      "Total crops: 2\n",
      "Saving image crop 0... label=Picture conf=0.916\n",
      "Saving image crop 1... label=Text conf=0.898\n",
      ">>> Processing image: pic 14.jpg\n",
      "\n",
      "0: 672x1024 1 Caption, 2 Page-footers, 2 Pictures, 1 Section-header, 20 Texts, 3001.7ms\n",
      "Speed: 17.2ms preprocess, 3001.7ms inference, 2.0ms postprocess per image at shape (1, 3, 672, 1024)\n",
      "Total crops: 26\n",
      "Saving image crop 0... label=Caption conf=0.903\n",
      "Saving image crop 1... label=Picture conf=0.910\n",
      "Saving image crop 2... label=Picture conf=0.606\n",
      "Saving image crop 3... label=Text conf=0.971\n",
      "Saving image crop 4... label=Text conf=0.971\n",
      "Saving image crop 5... label=Text conf=0.970\n",
      "Saving image crop 6... label=Text conf=0.970\n",
      "Saving image crop 7... label=Text conf=0.968\n",
      "Saving image crop 8... label=Text conf=0.968\n",
      "Saving image crop 9... label=Text conf=0.967\n",
      "Saving image crop 10... label=Text conf=0.966\n",
      "Saving image crop 11... label=Text conf=0.964\n",
      "Saving image crop 12... label=Text conf=0.963\n",
      "Saving image crop 13... label=Text conf=0.962\n",
      "Saving image crop 14... label=Text conf=0.960\n",
      "Saving image crop 15... label=Text conf=0.953\n",
      "Saving image crop 16... label=Text conf=0.952\n",
      "Saving image crop 17... label=Text conf=0.949\n",
      "Saving image crop 18... label=Text conf=0.923\n",
      "Saving image crop 19... label=Text conf=0.922\n",
      "Saving image crop 20... label=Text conf=0.880\n",
      "Saving image crop 21... label=Text conf=0.873\n",
      "Saving image crop 22... label=Text conf=0.726\n",
      "Saving image crop 23... label=Page-footer conf=0.711\n",
      "Saving image crop 24... label=Page-footer conf=0.690\n",
      "Saving image crop 25... label=Section-header conf=0.853\n",
      ">>> Processing image: pic 15.png\n",
      "\n",
      "0: 800x1024 (no detections), 3507.0ms\n",
      "Speed: 12.5ms preprocess, 3507.0ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 1024)\n",
      "Total crops: 0\n",
      ">>> Processing image: pic 16.jpg\n",
      "\n",
      "0: 1024x800 2 Pictures, 1 Section-header, 2 Texts, 3408.4ms\n",
      "Speed: 15.1ms preprocess, 3408.4ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 800)\n",
      "Total crops: 5\n",
      "Saving image crop 0... label=Picture conf=0.716\n",
      "Saving image crop 1... label=Picture conf=0.580\n",
      "Saving image crop 2... label=Text conf=0.767\n",
      "Saving image crop 3... label=Text conf=0.702\n",
      "Saving image crop 4... label=Section-header conf=0.770\n",
      ">>> Processing image: pic 17.png\n",
      "\n",
      "0: 896x1024 1 List-item, 1 Section-header, 3 Tables, 1 Text, 3693.9ms\n",
      "Speed: 14.6ms preprocess, 3693.9ms inference, 0.0ms postprocess per image at shape (1, 3, 896, 1024)\n",
      "Total crops: 3\n",
      "Saving image crop 0... label=Text conf=0.526\n",
      "Saving image crop 1... label=List-item conf=0.602\n",
      "Saving image crop 2... label=Section-header conf=0.747\n",
      ">>> Processing image: pic 18.jpg\n",
      "\n",
      "0: 736x1024 (no detections), 2750.7ms\n",
      "Speed: 0.0ms preprocess, 2750.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1024)\n",
      "Total crops: 0\n",
      ">>> Processing image: pic 19.png\n",
      "\n",
      "0: 1024x544 (no detections), 1732.0ms\n",
      "Speed: 7.4ms preprocess, 1732.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 544)\n",
      "Total crops: 0\n",
      ">>> Processing image: pic 2.jpeg\n",
      "\n",
      "0: 416x1024 1 List-item, 1 Table, 1 Text, 1454.8ms\n",
      "Speed: 0.0ms preprocess, 1454.8ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 1024)\n",
      "Total crops: 2\n",
      "Saving image crop 0... label=Text conf=0.663\n",
      "Saving image crop 1... label=List-item conf=0.790\n",
      ">>> Processing image: pic 20.jpg\n",
      "\n",
      "0: 1024x864 1 Picture, 2 Texts, 3779.5ms\n",
      "Speed: 15.5ms preprocess, 3779.5ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 864)\n",
      "Total crops: 3\n",
      "Saving image crop 0... label=Picture conf=0.800\n",
      "Saving image crop 1... label=Text conf=0.880\n",
      "Saving image crop 2... label=Text conf=0.816\n",
      ">>> Processing image: pic 21.png\n",
      "\n",
      "0: 416x1024 1 Picture, 1284.1ms\n",
      "Speed: 6.0ms preprocess, 1284.1ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 1024)\n",
      "Total crops: 1\n",
      "Saving image crop 0... label=Picture conf=0.833\n",
      ">>> Processing image: pic 22.jpg\n",
      "\n",
      "0: 1024x672 1 Page-footer, 1 Page-header, 1 Section-header, 6 Texts, 2543.4ms\n",
      "Speed: 7.6ms preprocess, 2543.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 672)\n",
      "Total crops: 9\n",
      "Saving image crop 0... label=Page-footer conf=0.511\n",
      "Saving image crop 1... label=Text conf=0.975\n",
      "Saving image crop 2... label=Text conf=0.970\n",
      "Saving image crop 3... label=Text conf=0.959\n",
      "Saving image crop 4... label=Text conf=0.938\n",
      "Saving image crop 5... label=Text conf=0.891\n",
      "Saving image crop 6... label=Text conf=0.821\n",
      "Saving image crop 7... label=Page-header conf=0.511\n",
      "Saving image crop 8... label=Section-header conf=0.735\n",
      ">>> Processing image: pic 23.png\n",
      "\n",
      "0: 480x1024 1 Table, 1930.9ms\n",
      "Speed: 9.8ms preprocess, 1930.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 1024)\n",
      "Total crops: 0\n",
      ">>> Processing image: pic 24.png\n",
      "\n",
      "0: 1024x800 2 Pictures, 4 Section-headers, 9 Texts, 3210.8ms\n",
      "Speed: 8.0ms preprocess, 3210.8ms inference, 5.8ms postprocess per image at shape (1, 3, 1024, 800)\n",
      "Total crops: 15\n",
      "Saving image crop 0... label=Picture conf=0.956\n",
      "Saving image crop 1... label=Picture conf=0.923\n",
      "Saving image crop 2... label=Text conf=0.977\n",
      "Saving image crop 3... label=Text conf=0.976\n",
      "Saving image crop 4... label=Text conf=0.973\n",
      "Saving image crop 5... label=Text conf=0.963\n",
      "Saving image crop 6... label=Text conf=0.962\n",
      "Saving image crop 7... label=Text conf=0.936\n",
      "Saving image crop 8... label=Text conf=0.891\n",
      "Saving image crop 9... label=Text conf=0.879\n",
      "Saving image crop 10... label=Text conf=0.659\n",
      "Saving image crop 11... label=Section-header conf=0.915\n",
      "Saving image crop 12... label=Section-header conf=0.814\n",
      "Saving image crop 13... label=Section-header conf=0.731\n",
      "Saving image crop 14... label=Section-header conf=0.537\n",
      ">>> Processing image: pic 25.png\n",
      "\n",
      "0: 1024x768 2 Page-footers, 1 Picture, 10 Texts, 3080.3ms\n",
      "Speed: 15.9ms preprocess, 3080.3ms inference, 8.3ms postprocess per image at shape (1, 3, 1024, 768)\n",
      "Total crops: 13\n",
      "Saving image crop 0... label=Picture conf=0.878\n",
      "Saving image crop 1... label=Page-footer conf=0.643\n",
      "Saving image crop 2... label=Page-footer conf=0.630\n",
      "Saving image crop 3... label=Text conf=0.978\n",
      "Saving image crop 4... label=Text conf=0.976\n",
      "Saving image crop 5... label=Text conf=0.974\n",
      "Saving image crop 6... label=Text conf=0.968\n",
      "Saving image crop 7... label=Text conf=0.967\n",
      "Saving image crop 8... label=Text conf=0.966\n",
      "Saving image crop 9... label=Text conf=0.949\n",
      "Saving image crop 10... label=Text conf=0.930\n",
      "Saving image crop 11... label=Text conf=0.589\n",
      "Saving image crop 12... label=Text conf=0.527\n",
      ">>> Processing image: pic 26.png\n",
      "\n",
      "0: 1024x704 1 Picture, 1 Section-header, 5 Texts, 2963.2ms\n",
      "Speed: 13.2ms preprocess, 2963.2ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 704)\n",
      "Total crops: 7\n",
      "Saving image crop 0... label=Picture conf=0.965\n",
      "Saving image crop 1... label=Text conf=0.954\n",
      "Saving image crop 2... label=Text conf=0.950\n",
      "Saving image crop 3... label=Text conf=0.920\n",
      "Saving image crop 4... label=Text conf=0.886\n",
      "Saving image crop 5... label=Text conf=0.807\n",
      "Saving image crop 6... label=Section-header conf=0.811\n",
      ">>> Processing image: pic 27.png\n",
      "\n",
      "0: 512x1024 1 Section-header, 1 Table, 2246.1ms\n",
      "Speed: 0.0ms preprocess, 2246.1ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 1024)\n",
      "Total crops: 1\n",
      "Saving image crop 0... label=Section-header conf=0.565\n",
      ">>> Processing image: pic 28.png\n",
      "\n",
      "0: 1024x1024 1 Section-header, 4407.5ms\n",
      "Speed: 21.3ms preprocess, 4407.5ms inference, 9.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Total crops: 1\n",
      "Saving image crop 0... label=Section-header conf=0.605\n",
      ">>> Processing image: pic 3.jpg\n",
      "\n",
      "0: 1024x672 2 Pictures, 1 Section-header, 6 Texts, 2635.8ms\n",
      "Speed: 15.2ms preprocess, 2635.8ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 672)\n",
      "Total crops: 9\n",
      "Saving image crop 0... label=Picture conf=0.901\n",
      "Saving image crop 1... label=Picture conf=0.660\n",
      "Saving image crop 2... label=Text conf=0.928\n",
      "Saving image crop 3... label=Text conf=0.873\n",
      "Saving image crop 4... label=Text conf=0.746\n",
      "Saving image crop 5... label=Text conf=0.702\n",
      "Saving image crop 6... label=Text conf=0.663\n",
      "Saving image crop 7... label=Text conf=0.650\n",
      "Saving image crop 8... label=Section-header conf=0.852\n",
      ">>> Processing image: pic 5.png\n",
      "\n",
      "0: 1024x736 1 Picture, 4 Texts, 2725.1ms\n",
      "Speed: 13.3ms preprocess, 2725.1ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 736)\n",
      "Total crops: 5\n",
      "Saving image crop 0... label=Picture conf=0.553\n",
      "Saving image crop 1... label=Text conf=0.917\n",
      "Saving image crop 2... label=Text conf=0.861\n",
      "Saving image crop 3... label=Text conf=0.647\n",
      "Saving image crop 4... label=Text conf=0.623\n",
      ">>> Processing image: pic 6.jpg\n",
      "\n",
      "0: 640x1024 1 Caption, 2 Page-footers, 3 Pictures, 3 Section-headers, 21 Texts, 2539.8ms\n",
      "Speed: 12.1ms preprocess, 2539.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 1024)\n",
      "Total crops: 30\n",
      "Saving image crop 0... label=Caption conf=0.583\n",
      "Saving image crop 1... label=Picture conf=0.932\n",
      "Saving image crop 2... label=Picture conf=0.925\n",
      "Saving image crop 3... label=Picture conf=0.878\n",
      "Saving image crop 4... label=Text conf=0.971\n",
      "Saving image crop 5... label=Text conf=0.971\n",
      "Saving image crop 6... label=Text conf=0.964\n",
      "Saving image crop 7... label=Text conf=0.964\n",
      "Saving image crop 8... label=Text conf=0.962\n",
      "Saving image crop 9... label=Text conf=0.959\n",
      "Saving image crop 10... label=Text conf=0.957\n",
      "Saving image crop 11... label=Text conf=0.956\n",
      "Saving image crop 12... label=Text conf=0.954\n",
      "Saving image crop 13... label=Text conf=0.953\n",
      "Saving image crop 14... label=Text conf=0.947\n",
      "Saving image crop 15... label=Text conf=0.923\n",
      "Saving image crop 16... label=Text conf=0.921\n",
      "Saving image crop 17... label=Text conf=0.905\n",
      "Saving image crop 18... label=Text conf=0.872\n",
      "Saving image crop 19... label=Text conf=0.860\n",
      "Saving image crop 20... label=Text conf=0.852\n",
      "Saving image crop 21... label=Text conf=0.670\n",
      "Saving image crop 22... label=Text conf=0.621\n",
      "Saving image crop 23... label=Text conf=0.620\n",
      "Saving image crop 24... label=Text conf=0.549\n",
      "Saving image crop 25... label=Page-footer conf=0.579\n",
      "Saving image crop 26... label=Page-footer conf=0.571\n",
      "Saving image crop 27... label=Section-header conf=0.861\n",
      "Saving image crop 28... label=Section-header conf=0.683\n",
      "Saving image crop 29... label=Section-header conf=0.519\n",
      ">>> Processing image: pic 8.png\n",
      "\n",
      "0: 1024x992 1 Picture, 5 Texts, 5347.3ms\n",
      "Speed: 5.5ms preprocess, 5347.3ms inference, 14.4ms postprocess per image at shape (1, 3, 1024, 992)\n",
      "Total crops: 6\n",
      "Saving image crop 0... label=Picture conf=0.619\n",
      "Saving image crop 1... label=Text conf=0.808\n",
      "Saving image crop 2... label=Text conf=0.698\n",
      "Saving image crop 3... label=Text conf=0.674\n",
      "Saving image crop 4... label=Text conf=0.551\n",
      "Saving image crop 5... label=Text conf=0.511\n"
     ]
    }
   ],
   "source": [
    "# === Fixed cropping + wrapper that uses your existing pipeline functions ===\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Set this to the folder that contains your images (change if necessary)\n",
    "IMAGES_DIR = r\"D:\\OCR session\"     # <-- update if your images are in a different folder\n",
    "OUTPUT_DIR_ROOT = r\"D:\\OCR\\Pipeline\\Paragraph_crops\"  # where crops will be saved\n",
    "\n",
    "# If you already defined these functions in previous cells, this wrapper will use them:\n",
    "# perform_prediction, extract_bboxes, nms_classwise, merge_text_boxes,\n",
    "# resolve_cross_class_conflicts, deduplicate_boxes\n",
    "# Also make sure `model` (YOLOv10 instance) is already created in your session.\n",
    "\n",
    "def get_layout_bboxes_yolo(page_image, model,\n",
    "                           pre_conf_thresh=0.5,\n",
    "                           class_nms_iou=0.3,\n",
    "                           merge_text_iou=0.1,\n",
    "                           cross_iou_thresh=0.9999,\n",
    "                           dedup_iou=0.2,\n",
    "                           final_conf_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Returns cleaned layout boxes for a PIL page_image using existing pipeline functions.\n",
    "    Assumes perform_prediction() and pipeline helper functions are defined in the session.\n",
    "    \"\"\"\n",
    "    # 1) run model (perform_prediction accepts PIL image)\n",
    "    raw_results = perform_prediction(page_image, model, imgsz=1024, conf=0.5, iou=0.45, device=\"cpu\")\n",
    "\n",
    "    # 2) extract boxes\n",
    "    boxes = extract_bboxes(raw_results)\n",
    "\n",
    "    # 3) pre-filter by confidence\n",
    "    boxes = [b for b in boxes if b[\"confidence\"] >= pre_conf_thresh]\n",
    "\n",
    "    # 4) class-wise NMS\n",
    "    boxes = nms_classwise(boxes, iou_thresh=class_nms_iou)\n",
    "\n",
    "    # 5) merge text fragments (if any)\n",
    "    if any(b[\"label\"].lower() == \"text\" for b in boxes):\n",
    "        boxes = merge_text_boxes(boxes, merge_iou=merge_text_iou)\n",
    "\n",
    "    # 6) resolve perfect cross-class conflicts\n",
    "    boxes = resolve_cross_class_conflicts(boxes, cross_iou_thresh=cross_iou_thresh)\n",
    "\n",
    "    # 7) extra deduplication per class\n",
    "    boxes = deduplicate_boxes(boxes, iou_thresh=dedup_iou)\n",
    "\n",
    "    # 8) final confidence filter\n",
    "    boxes = [b for b in boxes if b[\"confidence\"] >= final_conf_thresh]\n",
    "\n",
    "    # Return list of dicts: {\"bbox\":[x1,y1,x2,y2], \"label\":..., \"confidence\":...}\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def crop_text_areas(page_image):\n",
    "    \"\"\"\n",
    "    Takes a PIL page_image and returns list of PIL image crops for text areas.\n",
    "    Uses get_layout_bboxes_yolo(...) to get cleaned layout boxes.\n",
    "    \"\"\"\n",
    "    # call wrapper with the global `model` (must exist)\n",
    "    bboxes = get_layout_bboxes_yolo(page_image, model)\n",
    "\n",
    "    text_elements_list = []\n",
    "    for item in bboxes:\n",
    "        # correct unpacking order: x1, y1, x2, y2\n",
    "        x1, y1, x2, y2 = map(int, item[\"bbox\"])\n",
    "\n",
    "        # skip figures/tables (you asked to crop text areas only)\n",
    "        if item[\"label\"].lower() not in [\"figure\", \"table\"]:\n",
    "            # crop expects (left, upper, right, lower) => (x1,y1,x2,y2)\n",
    "            crop = page_image.crop((x1, y1, x2, y2))\n",
    "            # store crop and associated metadata (label, confidence)\n",
    "            text_elements_list.append({\n",
    "                \"crop\": crop,\n",
    "                \"label\": item[\"label\"],\n",
    "                \"confidence\": item[\"confidence\"],\n",
    "                \"bbox\": [x1, y1, x2, y2]\n",
    "            })\n",
    "    return text_elements_list\n",
    "\n",
    "\n",
    "def save_crops_for_page(page_image_filename):\n",
    "    \"\"\"\n",
    "    Input: filename (just the file name, not full path) inside IMAGES_DIR.\n",
    "    Saves crops to OUTPUT_DIR_ROOT/<filename_without_ext>/crop_i.jpg\n",
    "    \"\"\"\n",
    "    print(f\">>> Processing image: {page_image_filename}\")\n",
    "    image_path = os.path.join(IMAGES_DIR, page_image_filename)\n",
    "    page_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    text_elements_crops = crop_text_areas(page_image)\n",
    "    print(f\"Total crops: {len(text_elements_crops)}\")\n",
    "\n",
    "    output_dir = os.path.join(OUTPUT_DIR_ROOT, os.path.splitext(page_image_filename)[0])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for i, item in enumerate(text_elements_crops):\n",
    "        print(f\"Saving image crop {i}... label={item['label']} conf={item['confidence']:.3f}\")\n",
    "        crop = item[\"crop\"]\n",
    "        crop.save(os.path.join(output_dir, f\"crop_{i}.jpg\"))\n",
    "\n",
    "\n",
    "# === Main loop: process all images in IMAGES_DIR ===\n",
    "if __name__ == \"__main__\":\n",
    "    # filter for common image extensions\n",
    "    total_filenames = [f for f in os.listdir(IMAGES_DIR)\n",
    "                       if f.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\"))]\n",
    "    print(f\"Total page images: {len(total_filenames)}\")\n",
    "\n",
    "    for filename in total_filenames:\n",
    "        save_crops_for_page(filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".benchmark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
